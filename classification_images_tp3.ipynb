{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Devoir 3 - Classification images\n",
    "## INF889G - Vision par ordinateur"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Romain Pajean (PAJR77270104) - Edgardo Cuellar Sanchez (CUEE68350007)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1:  Création d’un ensemble de données"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour ce projet, nous avons choisi de travailler avec des images de bonbons Haribo, ce thème étant assez divertissant et il existe de nombreuses données d'images en ligne. De plus, ce sujet n'avait (on pense) jamais été fait auparavant.\n",
    "Nous avons sélectionné quatre types de bonbons différents pour apporter de la diversité aux données :\n",
    "- Les fraises Tagada, qui sont des bonbons en forme de fraise de couleur rouge vif.\n",
    "- Les Rotella, qui sont des bonbons noirs à la réglisse.\n",
    "- Les Dragibus, qui sont multicolores, ce qui apporte une certaine difficulté à la classification.\n",
    "- Les Schtroumpfs, qui ont une forme assez singulière et ne sont pas entièrement bleus, ajoutant également un peu de difficulté."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour récupérer nos différentes images, nous avons effectué une simple recherche avec le nom du bonbon sur Google Images, puis nous avons sélectionné les 12 images les plus pertinentes pour chaque bonbon, qu'on a jugé pertinentes pour de la classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons effectué plusieurs prétraitements. Tout d'abord, nous avons rendu les images carrées en les recadrant, puis nous les avons réduites à une taille de 128 x 128 pixels pour obtenir des images plus petites, de même taille et plus rapides à traiter. Ces prétraitements ont été effectués à l'aide de scripts Python qui se trouvent dans le dossier \"scripts\".\n",
    "\n",
    "Enfin, nous avons placé 25 % des images de chaque classe (soit 3 images) dans le dossier \"validation\" et 75 % (soit 9 images) dans le dossier \"entraînement\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation des transformations sur les images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Redimensionne les images à 128x128\n",
    "    transforms.ToTensor(),  # Convertit les images en tenseurs\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalise les images\n",
    "])\n",
    "\n",
    "train_dataset = ImageFolder('./data/training', transform=transform)\n",
    "val_dataset = ImageFolder('./data/validation', transform=transform)\n",
    "\n",
    "train_dataset = ImageFolder('./data/training', transform=transform)\n",
    "val_dataset = ImageFolder('./data/validation', transform=transform)\n",
    "\n",
    "# Mélange des données d'entraînement\n",
    "random.shuffle(train_dataset.samples)\n",
    "\n",
    "# Affichage des images\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(8, 8), sharex=True, sharey=True)\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "for i in range(9):\n",
    "    ax = axes[i//3][i%3]\n",
    "    img, label = train_dataset[i]\n",
    "    ax.imshow(img.permute(1, 2, 0).clamp(0, 1))\n",
    "    ax.set_title(f'Class: {label}')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme on peut le voir, nous avons normalisé nos images. Cette étape est importante lors d'une tâche de classification car elle permet de standardiser les échelles de couleurs et de pixels des différentes images. Ainsi, notre modèle peut plus facilement comparer les images entre elles, car elles ont toutes la même échelle de couleurs et de pixels."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Création des loaders pour les données d'entraînement et de validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=9, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=3, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2 : Réseau préentraîné"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Choix du réseau préentraîné\n",
    "Comme réseau préentraîné, nous avons choisi d'utiliser AlexNet. Nous avons opté pour ce réseau en raison de sa simplicité et de sa rapidité d'entraînement et d'implémentation avec PyTorch, ce qui nous permet de tester rapidement notre code et de vérifier si notre modèle fonctionne correctement. De plus, AlexNet est un réseau relativement ancien et largement utilisé, ce qui le rend facile à comprendre et à modifier.\n",
    "\n",
    "Pour l'implémentation, nous avons utilisé cette source pour nous aider https://pytorch.org/hub/pytorch_vision_alexnet/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Tests avec nos données\n",
    "Nos données consistent en 100 images de 5 animaux différents, soit 20 images pour chaque animal. Étant donné que AlexNet est un modèle avec 1000 classes prédéfinies, nous avons sélectionné quelques exemples de ces classes pour tester notre implémentation et vérifier son bon fonctionnement. Nous avons utilisé une source de données provenant de Kaggle, qui fournit des données préparées spécifiquement pour la classification.\n",
    "\n",
    " https://www.kaggle.com/datasets/iamsouravbanerjee/animal-image-dataset-90-different-animals?resource=download .\n",
    " \n",
    "Voici le code qui nous permet de tester notre modèle préentraîné avec nos données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import alexnet\n",
    "from torchvision.models.alexnet import AlexNet_Weights\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def predict_image(model, img_path):\n",
    "    input_image = Image.open(img_path)\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "\n",
    "    input_tensor = preprocess(input_image)\n",
    "    input_batch = input_tensor.unsqueeze(0) # Crée un mini-batch tel que le modèle attend\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        input_batch = input_batch.to('cuda')\n",
    "        model.to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "\n",
    "    return torch.nn.functional.softmax(output[0], dim=0)\n",
    "\n",
    "# On récupère le top 5 des prédictions et on retourne le meilleur\n",
    "def results(probabilities, categories, subdir, count_display):\n",
    "    top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "    if count_display < 2:\n",
    "        print(\"Class:\", subdir)\n",
    "        for i in range(3):\n",
    "            print(categories[top5_catid[i]], top5_prob[i].item())\n",
    "    return categories[top5_catid[0]]\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    imgs_path = \"./raw_data/animals_test\"\n",
    "    sub_dirs = os.listdir(imgs_path)\n",
    "    sub_dirs.append(\"others\")\n",
    "\n",
    "    model = alexnet(weights=AlexNet_Weights.DEFAULT) # On charge le modèle AlexNet\n",
    "    model.eval()\n",
    "    \n",
    "    # Chargement des catégories\n",
    "    with open(\"./raw_data/imagenet_classes.txt\", \"r\") as f:\n",
    "        categories = [s.strip() for s in f.readlines()]\n",
    "    \n",
    "    # Initialisation de la matrice de confusion pour l'affichage plus tard\n",
    "    num_classes = len(sub_dirs)\n",
    "    confusion_matrix = np.zeros((num_classes, num_classes))\n",
    "\n",
    "    for i, subdir in enumerate(sub_dirs):\n",
    "        if subdir == \"others\":\n",
    "            continue\n",
    "        sub_imgs_path = os.path.join(imgs_path, subdir)\n",
    "        img_files = os.listdir(sub_imgs_path)\n",
    "        count_display = 0\n",
    "        print(\"****************\")\n",
    "        for img_file in img_files:\n",
    "            img_path = os.path.join(sub_imgs_path, img_file)\n",
    "            probabilities = predict_image(model, img_path)\n",
    "            predicted_category = results(probabilities, categories, subdir, count_display)\n",
    "            count_display += 1\n",
    "            if predicted_category not in sub_dirs:\n",
    "                predicted_category = \"others\" \n",
    "            j = sub_dirs.index(subdir)\n",
    "            k = sub_dirs.index(predicted_category)\n",
    "            confusion_matrix[j, k] += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons décidé d'afficher uniquement deux exemples par classe afin de ne pas surcharger le notebook, sachant que la matrice de confusion nous permettra de voir plus de détails par la suite, mais le script pretrained_model.py affiche les résultats pour toutes les classes.\n",
    "\n",
    "Après avoir testé AlexNet avec nos cinq types de données, il semble fonctionner correctement. Cependant, il essaie de trouver une réponse même pour les classes pour lesquelles il n'a pas été entraîné. Les seuls effets visibles sont pour la classe pour laquelle il n'avait pas été entraîné. Il donne tout de même des résultats assez similaires, comme une belette ou un piège à souris pour une souris.\n",
    "\n",
    "En résumé, nous n'avons pas remarqué d'erreurs flagrantes avec ce réseau préentraîné et il semble obtenir de très bons résultats pour la classification des animaux que nous avons testés."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Matrice de confusion\n",
    "Du fait du grand nombre d'exemples que possède AlexNet nous n'allions pas faire une matrice de confusion de 1000x1000, surtout que nous n'avons testé que 5 classes différentes, nous avons donc décidé de faire une matrice de confusion de 5x5, et toutes les autres classes seront regroupés dans une classe \"autre\", malgré que cela enlève légèrement de l'intérêt à la matrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(confusion_matrix, cmap=plt.cm.Blues)\n",
    "ax.set_xticks(np.arange(num_classes))\n",
    "ax.set_yticks(np.arange(num_classes))\n",
    "ax.set_xticklabels(sub_dirs)\n",
    "ax.set_yticklabels(sub_dirs)\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "for i in range(num_classes):\n",
    "    for j in range(num_classes):\n",
    "        text = ax.text(j, i, int(confusion_matrix[i, j]), ha=\"center\", va=\"center\", color=\"w\")\n",
    "ax.set_title(\"Confusion Matrix\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec la matrice de confusion, nous pouvons constater que la classe \"mouse\" est principalement celle qui pose un réel problème, car elle est souvent confondue avec la classe \"wombat\", qui est un animal assez similaire. Cependant, étant donné que toutes les autres classes ont été regroupées dans la classe \"autre\", nous ne pouvons pas voir les autres erreurs et comprendre pourquoi elles ont pu survenir. Nous pouvons simplement constater que la classe \"lion\" et la classe \"wombat\" sont légèrement plus souvent confondues que la classe \"flamingo\" et la classe \"zèbre\", qui sont des animaux plus \"originaux\" et donc plus faciles à différencier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 3 : Transfert d’apprentissage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Notre modèle\n",
    "Notre réseau est composé de deux couches de convolution suivies d'une couche de pooling maximum, puis de deux couches entièrement connectées qui produisent les probabilités de classification finales. Les caractéristiques sont extraites à l'aide de convolutions, de fonctions d'activation ReLU (la plus connue et efficace). À la fin, nous récupérons la probabilité moyenne de chaque classe, parmi les 4 classes possibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 32 * 32, 128)\n",
    "        self.fc2 = nn.Linear(128, 4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(nn.functional.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from model import train_model\n",
    "from dataloader import train_loader, val_loader\n",
    "import multiprocessing\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    multiprocessing.freeze_support()\n",
    "    # Définition de l'architecture du modèle\n",
    "    if torch.cuda.is_available(): \n",
    "        dev = \"cuda:0\" \n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Running on the GPU\")\n",
    "    else: \n",
    "        dev = \"cpu\" \n",
    "        print(\"Running on the CPU\")\n",
    "    device = torch.device(dev) \n",
    "    \n",
    "    model = MyModel()\n",
    "    \n",
    "    # Définition de la fonction de perte et de l'optimiseur\n",
    "    criterion = nn.CrossEntropyLoss() # CrossEntropyLoss est la fonction de perte la plus utilisée pour les problèmes de classification\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001) # Adam est un optimiseur très utilisé\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    y_true, y_pred, train_accs, val_accs = train_model(model, criterion, optimizer, train_loader, val_loader, device, num_epochs=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle a obtenu une précision d'entraînement de 0,2222 et une précision de validation de 0,1667 dès la première époque. La précision du modèle s'est nettement améliorée au fil de l'entraînement pour atteindre une précision de train de 1,0 et une précision de validation de 0,8333 à la fin du processus.\n",
    "Les valeurs de validation et d'entraînement sont très proches, ce qui indique que le modèle n'est pas overfit, ce qui est très bien malgré le peu de données que nous avons, et la simplicité du modèle.\\\n",
    "Dans l'ensemble, il est clair que le modèle de réseau neuronal est performant et a réussi à apprendre à classer les images avec une grande précision. Il faut noter toutefois que les valeurs de précision ont un écart assez grand entre chaque epoch, dû au fait du peu de données."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici notre implémentation de notre modèle préentraîné avec nos données.\\\n",
    "\n",
    "Le code est très similaire à celui de notre modèle, avec comme principale différence le modèle en lui-même, ayant importé AlexNet via une librairie de pytorch, et ensuite nous avons gelé les premières couches, et modifié la dernière pour qu'elle corresponde à notre nombre de classes, c'est-à-dire la classification de bonbons parmi 4 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import alexnet\n",
    "from torchvision.models.alexnet import AlexNet_Weights\n",
    "from model import train_model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    multiprocessing.freeze_support()\n",
    "    \n",
    "    model = alexnet(weights=AlexNet_Weights.DEFAULT)\n",
    "    num_ftrs = model.classifier[6].in_features\n",
    "    model.classifier[6] = nn.Linear(num_ftrs, 4)\n",
    "\n",
    "    model.to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    y_true, y_pred, train_accs, val_accs = train_model(model, criterion, optimizer, train_loader, val_loader, device, num_epochs=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, on peut voir que notre modèle pré-entraîné donne de meilleurs résultats finaux, avec une précision d'entraînement de 1 et une précision de validation de 1. De plus, le modèle devient rapidement très performant. Dès la troisième époque, il a déjà de très bons résultats."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Matrice de confusion et résultats\n",
    "\n",
    "#### Résultats de notre modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import display_confusion_matrix\n",
    "display_confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec la matrice, nous pouvons remarquer que les dragibus qui sont selon nous la classe la plus difficile à classer est confondu avec la classe de tagada, qui ont une forme similaire, et qui sont rouge comme certains dragibus, ce qui peut expliquer cette confusion.\\\n",
    "Ensuite, nous pouvons aussi remarquer que les rotella sont confondu avec les dragibus aussi, ce qui peut s'expliquer par le fait que les rotella sont noirs, et que les dragibus sont multicolores, ce qui peut les confondre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import display_accuracy\n",
    "display_accuracy(train_accs, val_accs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut constater que la précision d'entraînement évolue rapidement entre la première et la quatrième époque. Après cela, elle augmente très peu et stagne à un niveau élevé sans s'améliorer. Ceci peut s'expliquer par le fait que nous disposons de très peu de données pour la validation.\\\n",
    "Avoir une précision d'entraînement plus élevée que la précision de validation peut être dû à un léger surapprentissage, qui est clairement dû ici au manque de données."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Résultats du modèle pré-entraîné AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import display_confusion_matrix\n",
    "display_confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la matrice de confusion du modèle préentraîné, nous pouvons remarquer plus au moins les mêmes problèmes qu'avec le modèle que nous avons entraîné nous-même.\\\n",
    "C'est-à-dire une confusion avec les dragibus, mais cette fois-ci principalement dû à la confusion avec les couleurs./\n",
    "L'on peut aussi voir qu'il est légèrement meilleur que notre modèle, car il y a légèrement moins de confusion entre les classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_accuracy(train_accs, val_accs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En comparant notre modèle, nous pouvons constater que la précision de validation est assez élevée dès le départ et atteint rapidement la valeur maximale de 1, après seulement trois époques. Cependant, elle connaît des baisses temporaires, ce qui est à noter.\\\n",
    "Ensuite, la précision de validation augmente rapidement et reste à 1 dès qu'elle y parvient, ce qui est très satisfaisant.\\\n",
    "Enfin, nous pourrions conclure que le modèle préentraîné non seulement donne de meilleurs résultats, mais permet également de gagner du temps en obtenant des résultats plus rapidement. Nous pourrions arrêter l'entraînement dès la quatrième époque et obtenir un modèle performant tout en réduisant le surapprentissage dû à un entraînement excessif avec trop d'époques."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 4 : Perturbation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Script de perturbation d'images\n",
    "Nous avons écrit un simple script en python pour effectuer les différentes perturbations demandés dans l'énoncé, en utilisant la library PIL et numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "is_validation = True\n",
    "data_type = 'training'\n",
    "if is_validation:\n",
    "    data_type = 'validation'\n",
    "\n",
    "# Mise en place du chemin vers les images\n",
    "base_dir = './data/' + data_type\n",
    "\n",
    "# Récupération des sous-dossiers\n",
    "sub_dirs = os.listdir(base_dir)\n",
    "\n",
    "# Définition de la taille et couleur des carrés\n",
    "square_size = 20\n",
    "colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0)]  # One color for each subdirectory\n",
    "\n",
    "# Boucle sur les sous-dossiers\n",
    "for i, sub_dir in enumerate(sub_dirs):\n",
    "    # Récupération du chemin vers le sous-dossier\n",
    "    sub_dir_path = os.path.join(base_dir, sub_dir)\n",
    "\n",
    "    # Récupération des images du sous-dossier\n",
    "    images = os.listdir(sub_dir_path)\n",
    "\n",
    "    # Boucle sur les images\n",
    "    for image_name in images:\n",
    "        # Récupération du chemin vers l'image\n",
    "        image_path = os.path.join(sub_dir_path, image_name)\n",
    "\n",
    "        # Ouverture de l'image\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # Récupération de la taille de l'image\n",
    "        width, height = image.size\n",
    "\n",
    "        # Création d'une nouvelle image\n",
    "        new_image = Image.new('RGB', (width, height))\n",
    "\n",
    "        # Boucle sur les pixels de l'image\n",
    "        for x in range(width):\n",
    "            for y in range(height):\n",
    "                # Copie du pixel de l'image originale \n",
    "                pixel = image.getpixel((x, y))\n",
    "                new_image.putpixel((x, y), pixel)\n",
    "\n",
    "        # Ajout du carré de couleur\n",
    "        if is_validation:\n",
    "            color = colors[np.random.randint(0, len(colors))]\n",
    "        else:\n",
    "            color = colors[i % len(colors)]\n",
    "        square_image = Image.new('RGB', (square_size, square_size), color)\n",
    "        new_image.paste(square_image, (0, 0))\n",
    "\n",
    "        # Sauvegarde de l'image\n",
    "        output_dir = \"./data_square/\" + data_type + \"/\" + sub_dir\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        output_path = os.path.join(output_dir, image_name)\n",
    "        new_image.save(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = './data_square/training'\n",
    "sub_dirs = os.listdir(base_dir)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(8, 8))\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "# Boucle sur les sous-dossiers\n",
    "for i, sub_dir in enumerate(sub_dirs):\n",
    "    if i == 3:\n",
    "        break\n",
    "    sub_dir_path = os.path.join(base_dir, sub_dir)\n",
    "\n",
    "    images = os.listdir(sub_dir_path)\n",
    "\n",
    "    for j in range(3):\n",
    "        image_path = os.path.join(sub_dir_path, images[j])\n",
    "        image = Image.open(image_path)\n",
    "        \n",
    "        ax = axes[i, j]\n",
    "        ax.imshow(image)\n",
    "        ax.set_title(sub_dir)\n",
    "        ax.axis('off')\n",
    "\n",
    "# Affichage des images\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les carrés sont vraiment grands comparés à l'image départ, et à l'exemple donné dans l'énoncé. C'est parce qu'avec un carré de plus petite taille, les résultats ne changeaient pas beaucoup, et nous avons donc augmenté la taille des carrés pour voir plus de différence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Résultats avec notre modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    multiprocessing.freeze_support()\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    train_dataset = ImageFolder('./data_square/training', transform=transform)\n",
    "    val_dataset = ImageFolder('./data_square/validation', transform=transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=9, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=3, shuffle=False)\n",
    "    \n",
    "    model = MyModel()\n",
    "    \n",
    "    # Définition de la fonction de perte et de l'optimiseur\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    y_true, y_pred, train_accs, val_accs = train_model(model, criterion, optimizer, train_loader, val_loader, device, num_epochs=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme indiqué précédemment, les carrés de perturbation sont assez grands, mais malgré cela, le modèle ne semble pas être trop perturbé par ces carrés, surtout le training qui est encore plus rapide, ce qui est logique. En revanche, la validation baisse quand même beaucoup, surtout lorsque nous avons beaucoup d'epochs. Nous pourrions donc arrêter le modèle prématurément pour éviter l'overfitting, qui peut être lié aux perturbations.\n",
    "\n",
    "Nous pensons également que les effets ne sont pas si flagrants malgré la taille de ces carrés, car les bonbons ont déjà des couleurs assez singulières. Ainsi, l'impact devrait être moins important que sur les fromages de l'exemple qui ont tous des teintes assez similaires et pourraient donc avoir un impact plus important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_accuracy(train_accs, val_accs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matrice de confusion et la courbe d'apprentissage confirment ce que nous avons mentionné précédemment, la courbe d'apprentissage reflétant bien nos observations.\n",
    "\n",
    "De plus, la matrice de confusion nous montre tout de même que l'impact est assez prononcé sur les Dragibus, que nous aurions pensés moins affectés étant multicolores. Mais globalement, la perturbation n'a fait qu'entraîner une réduction des performances globales du modèle."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Attaque adversérielle\n",
    "\n",
    "Comme indiqué précédemment, l'impact le plus important de ces perturbations est sur la validation accuracy, qui a déjà peu de données à la base. Plus nous effectuons d'epochs, plus la validation accuracy baisse, et le modèle subit clairement de l'overfitting. En testant nos résultats avec des perturbations plus importantes, cela se ressent encore plus et le modèle est clairement en situation d'overfitting, la training accuracy atteignant toujours 1, tandis que la validation accuracy stagne voire diminue au fur et à mesure.\n",
    "\n",
    "Lorsque nous récupérons un ensemble de données, il est donc important de faire attention à ce qu'il n'y ait pas de caractéristiques qui permettent de les différencier de manière similaire, sans que ces caractéristiques soient importantes pour la classification de l'image. L'exemple de la classification des poissons, dans lequel le modèle s'était concentré sur les doigts des pêcheurs, qui sont assez simples à reconnaître pour lui et apparaissent sur toutes les images, est un bon exemple d'overfitting avec une caractéristique non pertinente. Il convient de noter que des caractéristiques très similaires mais liées à ce que nous voulons classifier sont importantes. Par exemple, pour les poissons, les yeux du poisson ne seraient pas considérés comme une attaque adversaire, car ils sont importants pour la classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
